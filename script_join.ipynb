{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "fasta = \"ZMays/fasta\"\n",
    "tes = \"ZMays/tes\"\n",
    "groups = \"ZMays/Agrup\"\n",
    "\n",
    "# Iterate through the zip files\n",
    "for i in range(1, 11):\n",
    "\tzip_filename = f\"{fasta}/chromosome{i}.zip\"\n",
    "\n",
    "\t#Check if the zip file exists\n",
    "\tif os.path.exists(zip_filename):\n",
    "\t\ttry:\n",
    "\t\t\twith zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "\t\t\t\tzip_ref.extractall()\n",
    "\t\t\t\tprint(f\"Extração bem-sucedida {zip_filename}\")\n",
    "\t\texcept zipfile.BadZipFile:\n",
    "\t\t\tprint(f\"Erro: {zip_filename} não é um arquivo zip.\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Ocorreu um erro durante a extração {zip_filename}: {e}\")\n",
    "\telse:\n",
    "\t\tprint(f\"Warning: {zip_filename} não encontrado.\")\n",
    "\n",
    "# Use glob to find all .zip files\n",
    "zip_files = glob.glob(\"*.zip\")\n",
    "\n",
    "# Iterate through the found zip files\n",
    "for zip_filename in zip_files:\n",
    "\ttry:\n",
    "\t\tos.remove(zip_filename)\n",
    "\t\tprint(f\"Removido com sucesso {zip_filename}\")\n",
    "\texcept OSError as e:\n",
    "\t\tprint(f\"Erro: {zip_filename}: {e}\")\n",
    "\t\t\n",
    "file_path = (tes + \"/TEAnnotationFinal_Helitron.gff3\")\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "\t# Use list comprehension para pegar as primeiras 10 linhas\n",
    "\tfor i, line in enumerate(file):\n",
    "\t\tprint(line.strip())  # strip() remove espaços extras e quebras de linha duplicadas\n",
    "\t\tif i == 9:  # Exibe apenas as primeiras 10 linhas\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "def add_gff3_to_dataframe(df, file_path):\n",
    "\t\"\"\"\n",
    "\tLê um arquivo GFF3, extrai os dados e adiciona ao DataFrame existente.\n",
    "\n",
    "\tParâmetros:\n",
    "\t\tdf (pd.DataFrame): DataFrame existente para o qual os dados serão adicionados.\n",
    "\t\tfile_path (str): Caminho do arquivo GFF3.\n",
    "\n",
    "\tRetorna:\n",
    "\t\tpd.DataFrame: DataFrame atualizado com os novos dados.\n",
    "\t\"\"\"\n",
    "\t# Definir os nomes das colunas do arquivo GFF3\n",
    "\tcolumn_names = [\"Chr\", \"SourceAnnotation\", \"COS\", \"Start\", \"End\", \"Score\", \"Strand\", \"Phase\", \"Attributes\"]\n",
    "\n",
    "\t# Lista para armazenar os dados do novo arquivo\n",
    "\tdata = []\n",
    "\n",
    "\ttry:\n",
    "\t\twith open(file_path, \"r\") as file:\n",
    "\t\t\tfor line in file:\n",
    "\t\t\t\tif not line.startswith(\"#\"):  # Ignorar linhas de comentários\n",
    "\t\t\t\t\tparts = line.strip().split(\"\\t\")\n",
    "\t\t\t\t\tif len(parts) == len(column_names):  # Verificar o número correto de colunas\n",
    "\t\t\t\t\t\tdata.append(parts)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tprint(f\"Warning: Skipping line with incorrect number of columns: {line.strip()}\")\n",
    "\n",
    "\t\t# Criar um DataFrame com os dados lidos\n",
    "\t\tnew_df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "\t\t# Adicionar os novos dados ao DataFrame existente\n",
    "\t\tupdated_df = pd.concat([df, new_df], ignore_index=True)\n",
    "\t\treturn updated_df\n",
    "\n",
    "\texcept FileNotFoundError:\n",
    "\t\tprint(f\"Error: File not found at {file_path}\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"An error occurred: {e}\")\n",
    "\n",
    "\treturn df  # Retorna o DataFrame original caso haja um erro\n",
    "\n",
    "helitron = \"ZMays/tes/TEAnnotationFinal_Helitron.gff3\"\n",
    "line = \"ZMays/tes/TEAnnotationFinal_LINE.gff3\"\n",
    "ltr = \"ZMays/tes/TEAnnotationFinal_LTR.gff3\"\n",
    "mite = \"ZMays/tes/TEAnnotationFinal_MITE.gff3\"\n",
    "sine = \"ZMays/tes/TEAnnotationFinal_SINE.gff3\"\n",
    "tir = \"ZMays/tes/TEAnnotationFinal_TIR.gff3\"\n",
    "\n",
    "df = \"\"\n",
    "df = pd.DataFrame(columns=[\"Chr\", \"SourceAnnotation\", \"COS\", \"Start\", \"End\", \"Score\", \"Strand\", \"Phase\", \"Attributes\"])\n",
    "\n",
    "df = add_gff3_to_dataframe(df, helitron)\n",
    "df = add_gff3_to_dataframe(df, line)\n",
    "df = add_gff3_to_dataframe(df, ltr)\n",
    "df = add_gff3_to_dataframe(df, mite)\n",
    "df = add_gff3_to_dataframe(df, sine)\n",
    "df = add_gff3_to_dataframe(df, tir)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "#df = df[df['Strand'] == '+']\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"\\n\\nNúmero de entradas: {len(df)}\")\n",
    "\n",
    "# Ordernar o DataFrame\n",
    "df['Start'] = df['Start'].astype(int)\n",
    "df['End'] = df['End'].astype(int)\n",
    "df_sorted = df.sort_values(by=['Chr', 'Start'])\n",
    "\n",
    "# Reset the index of the sorted DataFrame\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_sorted.head())\n",
    "print(f\"\\n\\nNúmero de entradas: {len(df_sorted)}\")\n",
    "\n",
    "# Lista dos tipos permitidos\n",
    "types = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "\n",
    "# Filtrar o DataFrame para manter apenas os tipos permitidos\n",
    "df = df[df[\"Chr\"].isin(types)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Sumarização\n",
    "summary = df[\"Chr\"].value_counts()\n",
    "\n",
    "for index, value in summary.items():\n",
    "\tprint(f\"{index}: {value}\")\n",
    "\n",
    "print(f\"\\n\\nNúmero de entradas: {len(df)}\")\n",
    "\n",
    "dfs_metrics = []\n",
    "\n",
    "i = 0\n",
    "for f in glob.glob(groups + \"/*.csv\"):\n",
    "\tdf_metric = pd.read_csv(f)\n",
    "\tdf_metric = df_metric.sort_values(by='nameseq')\n",
    "\n",
    "\tcolumns = [f\"{f.replace('ZeaMays-TEsClassification/ZMays/Agrup/', '').replace('.csv', '')}_{column}\" for column in df_metric.columns]\n",
    "\tif i != 0:\n",
    "\t\tdf_metric.drop(columns=['nameseq'], inplace=True)\n",
    "\t\tcolumns.remove(columns[0])\n",
    "\telse:\n",
    "\t\tcolumns[0] = 'nameseq'\n",
    "\n",
    "\tdf_metric.columns = columns\n",
    "\n",
    "\tdf_metric.reset_index(drop=True, inplace=True)\n",
    "\tdfs_metrics.append(df_metric)\n",
    "\ti += 1\n",
    "\n",
    "df_metrics = pd.concat(dfs_metrics, axis=1)\n",
    "\n",
    "mask = df_metrics['nameseq'].apply(lambda x: isinstance(x, str))\n",
    "df_filtrado = df_metrics.loc[mask]\n",
    "\n",
    "mascara = df_metrics['nameseq'].map(type) == str\n",
    "\n",
    "# Aplica a máscara para filtrar o DataFrame\n",
    "df_metrics_filtrado = df_metrics[mascara]\n",
    "\n",
    "print(\"DataFrame original:\")\n",
    "print(df_metrics)\n",
    "print(\"\\nDataFrame filtrado (apenas strings em 'nameseq'):\")\n",
    "print(df_metrics_filtrado)\n",
    "\n",
    "def juntar_csvs_em_dataframe(arquivo_zip1: str, arquivo_zip2: str, arquivo_zip3: str, arquivo_zip4: str, arquivo_zip5: str) -> pd.DataFrame | None:\n",
    "\ttry:\n",
    "\t\t#with zipfile.ZipFile(arquivo_zip1, 'r') as zipfile1:\n",
    "\t\t#  with zipfile.ZipFile(arquivo_zip2, 'r') as zipfile2:\n",
    "\t\t#    with zipfile.ZipFile(arquivo_zip3, 'r') as zipfile3:\n",
    "\t\t#      with zipfile.ZipFile(arquivo_zip4, 'r') as zipfile4:\n",
    "\t\t#        with zipfile.ZipFile(arquivo_zip5, 'r') as zipfile5:\n",
    "\t\t#zipfile1.extractall()\n",
    "\t\t#rint(f\"Extração bem-sucedida {arquivo_zip1}\")\n",
    "\t\t#ipfile2.extractall()\n",
    "\t\t#print(f\"Extração bem-sucedida {arquivo_zip2}\")\n",
    "\t\t#zipfile3.extractall()\n",
    "\t\t#print(f\"Extração bem-sucedida {arquivo_zip3}\")\n",
    "\t\t#print(f\"Extração bem-sucedida {arquivo_zip4}\")\n",
    "\t\t#zipfile5.extractall()\n",
    "\t\t#print(f\"Extração bem-sucedida {arquivo_zip5}\")\n",
    "\n",
    "\t\tarquivo_csv1 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_1_de_5.csv'\n",
    "\t\tarquivo_csv2 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_2_de_5.csv'\n",
    "\t\tarquivo_csv3 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_3_de_5.csv'\n",
    "\t\tarquivo_csv4 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_4_de_5.csv'\n",
    "\t\tarquivo_csv5 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_5_de_5.csv'\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tprint(f\"\\nLendo o primeiro arquivo para junção: '{arquivo_csv1}'...\")\n",
    "\t\t\tdf1 = pd.read_csv(arquivo_csv1)\n",
    "\t\t\t# Seleciona apenas as colunas cujo nome tem tamanho 6 e começa com A, T, C ou G\n",
    "\t\t\tcolunas_kmer = [col for col in df1.columns if all(c in {'A', 'T', 'C', 'G'} for c in col[-6:])]\n",
    "\t\t\tdf1.drop(columns=colunas_kmer, inplace=True)\n",
    "\t\t\tprint(f\"Dimensões de '{os.path.basename(arquivo_csv1)}': {df1.shape[0]} linhas, {df1.shape[1]} colunas\")\n",
    "\n",
    "\t\t\tprint(f\"Lendo o segundo arquivo para junção: '{arquivo_csv2}'...\")\n",
    "\t\t\tdf2 = pd.read_csv(arquivo_csv2)\n",
    "\t\t\t# Seleciona apenas as colunas cujo nome tem tamanho 6 e começa com A, T, C ou G\n",
    "\t\t\tcolunas_kmer = [col for col in df2.columns if all(c in {'A', 'T', 'C', 'G'} for c in col[-6:])]\n",
    "\t\t\tdf2.drop(columns=colunas_kmer, inplace=True)\n",
    "\t\t\tprint(f\"Dimensões de '{os.path.basename(arquivo_csv2)}': {df2.shape[0]} linhas, {df2.shape[1]} colunas\")\n",
    "\n",
    "\t\t\tprint(f\"\\nLendo o primeiro arquivo para junção: '{arquivo_csv3}'...\")\n",
    "\t\t\tdf3 = pd.read_csv(arquivo_csv3)\n",
    "\t\t\t# Seleciona apenas as colunas cujo nome tem tamanho 6 e começa com A, T, C ou G\n",
    "\t\t\tcolunas_kmer = [col for col in df3.columns if all(c in {'A', 'T', 'C', 'G'} for c in col[-6:])]\n",
    "\t\t\tdf3.drop(columns=colunas_kmer, inplace=True)\n",
    "\t\t\tprint(f\"Dimensões de '{os.path.basename(arquivo_csv3)}': {df3.shape[0]} linhas, {df3.shape[1]} colunas\")\n",
    "\n",
    "\t\t\tprint(f\"Lendo o segundo arquivo para junção: '{arquivo_csv4}'...\")\n",
    "\t\t\tdf4 = pd.read_csv(arquivo_csv4)\n",
    "\t\t\t# Seleciona apenas as colunas cujo nome tem tamanho 6 e começa com A, T, C ou G\n",
    "\t\t\tcolunas_kmer = [col for col in df4.columns if all(c in {'A', 'T', 'C', 'G'} for c in col[-6:])]\n",
    "\t\t\tdf4.drop(columns=colunas_kmer, inplace=True)\n",
    "\t\t\tprint(f\"Dimensões de '{os.path.basename(arquivo_csv4)}': {df4.shape[0]} linhas, {df4.shape[1]} colunas\")\n",
    "\n",
    "\t\t\tprint(f\"\\nLendo o primeiro arquivo para junção: '{arquivo_csv5}'...\")\n",
    "\t\t\tdf5 = pd.read_csv(arquivo_csv5)\n",
    "\t\t\t# Seleciona apenas as colunas cujo nome tem tamanho 6 e começa com A, T, C ou G\n",
    "\t\t\tcolunas_kmer = [col for col in df5.columns if all(c in {'A', 'T', 'C', 'G'} for c in col[-6:])]\n",
    "\t\t\tdf5.drop(columns=colunas_kmer, inplace=True)\n",
    "\t\t\tprint(f\"Dimensões de '{os.path.basename(arquivo_csv5)}': {df5.shape[0]} linhas, {df5.shape[1]} colunas\")\n",
    "\n",
    "\t\t\tdf_concatenado = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "\t\t\tprint(f\"Dimensões do DataFrame final concatenado: {df_concatenado.shape[0]} linhas, {df_concatenado.shape[1]} colunas\")\n",
    "\t\t\tprint(f\"Arquivos '{arquivo_csv1}' e '{arquivo_csv2}' juntados em um DataFrame com sucesso!\")\n",
    "\t\t\treturn df_concatenado\n",
    "\t\texcept FileNotFoundError:\n",
    "\t\t\tprint(f\"Erro: Um ou todos os arquivos CSV ('{arquivo_csv1}', '{arquivo_csv2}', '{arquivo_csv3}', '{arquivo_csv4}', '{arquivo_csv5}') não foram encontrados.\")\n",
    "\t\treturn None\n",
    "\n",
    "\t#except zipfile.BadZipFile:\n",
    "\t  #print(f\"Erro: não é um arquivo zip.\")\n",
    "\texcept Exception as e:\n",
    "\t  print(f\"Ocorreu um erro durante a extração: {e}\")\n",
    "\n",
    "\n",
    "def juntar_dataframes_inner(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame | None:\n",
    "\n",
    "\tif 'nameseq' not in df1.columns or 'nameseq' not in df2.columns:\n",
    "\t\tprint(\"Erro: A coluna 'nameseq' não está presente em ambos os DataFrames.\")\n",
    "\t\treturn None\n",
    "\n",
    "\tdf_juntado = pd.merge(df1, df2, on='nameseq', how='inner')\n",
    "\tprint(f\"DataFrames juntados com 'inner join' na coluna 'nameseq'. Dimensões do resultado: {df_juntado.shape}\")\n",
    "\treturn df_juntado\n",
    "\n",
    "arquivo_csv_parte1 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_1_de_5.zip'\n",
    "arquivo_csv_parte2 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_2_de_5.zip'\n",
    "arquivo_csv_parte3 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_3_de_5.zip'\n",
    "arquivo_csv_parte4 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_4_de_5.zip'\n",
    "arquivo_csv_parte5 = 'ZMays/Agrup/k-mer/divididos/k-mer_agrupado_parte_5_de_5.zip'\n",
    "\n",
    "\n",
    "\n",
    "df_kmer = juntar_csvs_em_dataframe(arquivo_csv_parte1, arquivo_csv_parte2, arquivo_csv_parte3, arquivo_csv_parte4, arquivo_csv_parte5)\n",
    "\n",
    "# Use glob to find all .zip files\n",
    "zip_files = glob.glob(\"*.zip\")\n",
    "\n",
    "# Iterate through the found zip files\n",
    "for zip_filename in zip_files:\n",
    "\ttry:\n",
    "\t\tos.remove(zip_filename)\n",
    "\t\tprint(f\"Removido com sucesso {zip_filename}\")\n",
    "\texcept OSError as e:\n",
    "\t\tprint(f\"Erro: {zip_filename}: {e}\")\n",
    "\n",
    "df_kmer.head()\n",
    "\n",
    "df_all_metrics = juntar_dataframes_inner(df_metrics_filtrado, df_kmer)\n",
    "\n",
    "#df_all_metrics = df_metrics_filtrado.copy()\n",
    "\n",
    "print(df_all_metrics.head())\n",
    "\n",
    "print(df_all_metrics.shape)\n",
    "\n",
    "def join_metrics_inner(df_main, df_metrics_values):\n",
    "\n",
    "\tdf_metrics = df_metrics_values.copy()\n",
    "\n",
    "\t# Dividir 'nameseq' em colunas chave\n",
    "\ttry:\n",
    "\n",
    "\t\tsplit_data = df_metrics['nameseq'].str.split('_', expand=True, n=2)\n",
    "\n",
    "\t\tif split_data.shape[1] < 3:\n",
    "\t\t\traise ValueError(\n",
    "\t\t\t\t\"A coluna 'nameseq' em df_metrics_values não pôde ser dividida em 3 partes (Chr, Start, End) consistentemente.\"\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdf_metrics['_Chr_key_metric'] = split_data[0].astype(str)\n",
    "\t\tdf_metrics['_Start_key_metric'] = split_data[1].astype(str)\n",
    "\t\tdf_metrics['_End_key_metric'] = split_data[2].astype(str)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\n",
    "\t\tprint(f\"Erro ao processar a coluna 'nameseq': {e}\")\n",
    "\t\tprint(\"Verifique o formato dos dados em 'nameseq'. Esperado: 'Chr_Start_End'.\")\n",
    "\n",
    "\t\traise\n",
    "\n",
    "\t# Identificar as colunas de métricas que serão transferidas/atualizadas\n",
    "\tmetric_value_cols = [\n",
    "\t\tcol for col in df_metrics.columns\n",
    "\t\tif col not in ['nameseq', '_Chr_key_metric', '_Start_key_metric', '_End_key_metric']\n",
    "\t]\n",
    "\n",
    "\t# Selecionar apenas as colunas chave e de métricas para a junção\n",
    "\tdf_metrics_to_join = df_metrics[\n",
    "\t\t['_Chr_key_metric', '_Start_key_metric', '_End_key_metric'] + metric_value_cols\n",
    "\t]\n",
    "\n",
    "\tdf_left = df_main.copy()\n",
    "\n",
    "\t# Adicionar colunas chave temporárias em df_left, convertendo para string para correspondência\n",
    "\tdf_left['_Chr_key_df'] = df_left['Chr'].astype(str)\n",
    "\tdf_left['_Start_key_df'] = df_left['Start'].astype(str)\n",
    "\tdf_left['_End_key_df'] = df_left['End'].astype(str)\n",
    "\n",
    "\tmerged_df = pd.merge(\n",
    "\t\tdf_left,\n",
    "\t\tdf_metrics_to_join,\n",
    "\t\tleft_on=['_Chr_key_df', '_Start_key_df', '_End_key_df'],\n",
    "\t\tright_on=['_Chr_key_metric', '_Start_key_metric', '_End_key_metric'],\n",
    "\t\thow='inner', # Especifica a junção interna\n",
    "\t\tsuffixes=('', '_newmetric')\n",
    "\t)\n",
    "\n",
    "\n",
    "\tfor metric_col_name in metric_value_cols:\n",
    "\n",
    "\t\tsuffixed_metric_col_name = metric_col_name + '_newmetric'\n",
    "\n",
    "\t\tif suffixed_metric_col_name in merged_df.columns:\n",
    "\n",
    "\t\t\tmerged_df[metric_col_name] = merged_df[suffixed_metric_col_name]\n",
    "\t\t\tmerged_df.drop(columns=[suffixed_metric_col_name], inplace=True)\n",
    "\n",
    "\tcols_to_drop = [\n",
    "\t\t'_Chr_key_df', '_Start_key_df', '_End_key_df',\n",
    "\t\t'_Chr_key_metric', '_Start_key_metric', '_End_key_metric' # Remover se ainda existirem\n",
    "\t]\n",
    "\n",
    "\texisting_cols_to_drop = [col for col in cols_to_drop if col in merged_df.columns]\n",
    "\tmerged_df.drop(columns=existing_cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "\treturn merged_df\n",
    "\n",
    "df_resultado_juncao = join_metrics_inner(df, df_all_metrics)\n",
    "print(df_resultado_juncao)\n",
    "\n",
    "df_met_seq = 'df_metricas_seq.csv'\n",
    "\n",
    "try:\n",
    "\tdf_resultado_juncao.to_csv(df_met_seq, index=False, encoding='utf-8')\n",
    "\tprint(f\"DataFrame salvo com sucesso como '{df_met_seq}'!\")\n",
    "except Exception as e:\n",
    "\tprint(f\"Ocorreu um erro ao salvar o DataFrame: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
