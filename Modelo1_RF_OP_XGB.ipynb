{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "\taccuracy_score,\n",
    "\tprecision_score,\n",
    "\trecall_score,\n",
    "\tf1_score,\n",
    "\tconfusion_matrix,\n",
    "\troc_curve,\n",
    "\tauc,\n",
    "\tclassification_report\n",
    ")\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55225588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_dados_csv(file):\n",
    "\n",
    "\ttry:\n",
    "\t\t\n",
    "\t\tdf_completo = pd.read_csv(file)\n",
    "\n",
    "\t\treturn df_completo\n",
    "\n",
    "\texcept FileNotFoundError:\n",
    "\n",
    "\t\tprint(f\"Erro: O arquivo '{file}' não foi encontrado.\")\n",
    "\t\tprint(\"Rode o ZeaMays para gerar o arquivo CSV.\")\n",
    "\t\treturn None\n",
    "\n",
    "\texcept KeyError as e:\n",
    "\n",
    "\t\tprint(f\"Erro: Uma ou mais colunas não encontradas no arquivo: {e}\")\n",
    "\t\treturn None\n",
    "\n",
    "\texcept Exception as e:\n",
    "\n",
    "\t\tprint(f\"Ocorreu um erro inesperado: {e}\")\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = \"df_metricas_seq.csv\"\n",
    "\n",
    "df_metricas = carregar_dados_csv(metricas)\n",
    "\n",
    "if df_metricas is not None:\n",
    "\tprint(\"\\nHead do DataFrame:\")\n",
    "\tprint(df_metricas.shape)\n",
    "\tprint(df_metricas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_colunas = [\"Chr\", \"SourceAnnotation\", \"Start\", \"End\", \"Score\", \"Strand\", \"Phase\", \"Attributes\", \"label\"]\n",
    "\n",
    "df_modificado = df_metricas.drop(columns=remover_colunas)\n",
    "\n",
    "print(\"\\nDataFrame após remover colunas:\")\n",
    "print(df_modificado.shape)\n",
    "print(df_modificado.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna_alvo = 'COS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verfica_unique (df, coluna):\n",
    "\t\n",
    "\tif coluna in df.columns:\n",
    "\n",
    "\t\tcontagem_valores = df[coluna].value_counts()\n",
    "\n",
    "\t\tprint(f\"Contagem de cada valor único na coluna '{coluna}':\")\n",
    "\t\tprint(contagem_valores)\n",
    "\n",
    "\t\tvalores_unicos = df[coluna].unique()\n",
    "\t\tprint(f\"\\nValores únicos na coluna '{coluna}':\")\n",
    "\t\tprint(valores_unicos)\n",
    "\n",
    "\t\tnumero_de_valores_unicos = df[coluna].nunique()\n",
    "\t\tprint(f\"\\nA coluna '{coluna}' possui {numero_de_valores_unicos} valores únicos distintos.\")\n",
    "\n",
    "\telse:\n",
    "\t\tprint(f\"A coluna '{coluna}' não foi encontrada no DataFrame.\")\n",
    "\t\tprint(\"Colunas disponíveis:\", df.columns.tolist())\n",
    "\n",
    "verfica_unique(df_modificado, coluna_alvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_elemento_transponivel(texto):\n",
    "  \n",
    "\ttry:\n",
    "\t\treturn texto.split('/')[1]\n",
    "\texcept IndexError:\n",
    "\t\treturn\n",
    "\t\n",
    "df_modificado[coluna_alvo] = df_modificado[coluna_alvo].apply(extrair_elemento_transponivel)\n",
    "\n",
    "verfica_unique(df_modificado, coluna_alvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bae107",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocorrencia = 1\n",
    "\n",
    "contagens = df_modificado[coluna_alvo].value_counts()\n",
    "\n",
    "valores_para_manter = contagens[contagens > ocorrencia].index\n",
    "\n",
    "df_filtrado = df_modificado[df_modificado[coluna_alvo].isin(valores_para_manter)]\n",
    "\n",
    "verfica_unique(df_filtrado, coluna_alvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Encontra todas as colunas que começam com o prefixo \"chaos_\"\n",
    "colunas_para_remover = [col for col in df_filtrado.columns if col.startswith('ZMays/Agrup\\chaos_')]\n",
    "\n",
    "# Cria um novo DataFrame sem essas colunas\n",
    "df_filtrado = df_filtrado.drop(columns=colunas_para_remover)\n",
    "\n",
    "print(\"Colunas removidas:\", colunas_para_remover)\n",
    "print(\"\\nNovo formato do DataFrame:\", df_filtrado.shape)\n",
    "print(\"Primeiras linhas do DataFrame limpo:\")\n",
    "print(df_filtrado.head())\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, feature_names, caminho_salvar):\n",
    "    \n",
    "    print(\"--- Treinando o Modelo Random Forest ---\")\n",
    "    modelo_rf = RandomForestClassifier(n_estimators=120, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "    modelo_rf.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Salvando o modelo em: {caminho_salvar}\")\n",
    "    joblib.dump(modelo_rf, caminho_salvar)\n",
    "    print(\"Modelo Random Forest treinado e salvo com sucesso!\")\n",
    "\n",
    "def random_forest_optuna(X_train, y_train, X_test, y_test, feature_names, caminho_salvar, n_trials=100):\n",
    "    \n",
    "    print(\"--- Otimizando e Treinando o Modelo Random Forest com Optuna ---\")\n",
    "    \n",
    "    def objetivo(trial):\n",
    "        \n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 50),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 32),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 32),\n",
    "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        modelo_rf = RandomForestClassifier(**params)\n",
    "        modelo_rf.fit(X_train, y_train)\n",
    "        y_pred = modelo_rf.predict(X_test)\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "\n",
    "    estudo = optuna.create_study(direction='maximize')\n",
    "    estudo.optimize(objetivo, n_trials=n_trials)\n",
    "    \n",
    "    melhores_params = estudo.best_params\n",
    "    print(f\"Melhores hiperparâmetros encontrados: {melhores_params}\")\n",
    "    \n",
    "    modelo_rf_optuna = RandomForestClassifier(**melhores_params, random_state=42, n_jobs=-1)\n",
    "    modelo_rf_optuna.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Salvando o modelo em: {caminho_salvar}\")\n",
    "    joblib.dump(modelo_rf_optuna, caminho_salvar)\n",
    "    print(\"Modelo Random Forest (Optuna) treinado e salvo com sucesso!\")\n",
    "\n",
    "def xgboost(X_train, y_train, feature_names, caminho_salvar):\n",
    "    \n",
    "    print(\"--- Treinando o Modelo XGBoost ---\")\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    params_xgb = {\n",
    "        'n_estimators': 120,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 4,\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "    }\n",
    "    if num_classes > 2:\n",
    "        params_xgb['objective'] = 'multi:softprob'\n",
    "        params_xgb['eval_metric'] = 'mlogloss'\n",
    "    else:\n",
    "        params_xgb['objective'] = 'binary:logistic'\n",
    "        params_xgb['eval_metric'] = 'logloss'\n",
    "        \n",
    "    modelo_xgb = XGBClassifier(**params_xgb)\n",
    "    modelo_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Salvando o modelo em: {caminho_salvar}\")\n",
    "    joblib.dump(modelo_xgb, caminho_salvar)\n",
    "    print(\"Modelo XGBoost treinado e salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d7d37",
   "metadata": {},
   "source": [
    "['Helitron' 'LINE' 'LTR' 'MITE' 'SINE' 'TIR']\n",
    "[0 1 2 3 4 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a25f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_coluna_alvo = 'COS'\n",
    "\n",
    "df_modelo = df_filtrado.copy()\n",
    "\n",
    "# Amostragem de 10% do DataFrame filtrado para um pseudo-treinamento\n",
    "# df_modelo = df_filtrado.sample(frac=0.02, random_state=42)\n",
    "\n",
    "if nome_coluna_alvo not in df_modelo.columns:\n",
    "\n",
    "\tprint(f\"ERRO: Coluna alvo '{nome_coluna_alvo}' não encontrada no DataFrame!\")\n",
    "\t\n",
    "else:\n",
    "\n",
    "\ty_original = df_modelo[nome_coluna_alvo]\n",
    "\tX_original = df_modelo.drop(columns=[nome_coluna_alvo])\n",
    "\t\n",
    "\tprint(f\"\\nVariável Alvo Selecionada: '{nome_coluna_alvo}' (Multiclasse)\")\n",
    "\tprint(\"-\" * 50)\n",
    "\n",
    "\t# Codificar Features Categóricas em X (One-Hot Encoding)\n",
    "\tcolunas_categoricas_X = X_original.select_dtypes(include=['object', 'category']).columns\n",
    "\tX_codificado = pd.get_dummies(X_original, columns=colunas_categoricas_X, drop_first=True)\n",
    "\tfeature_names = X_codificado.columns.tolist()\n",
    "\n",
    "\tprint(\"\\nFeatures (X) após One-Hot Encoding (primeiras linhas):\")\n",
    "\tprint(X_codificado.head())\n",
    "\tprint(f\"Número de features após encoding: {X_codificado.shape[1]}\")\n",
    "\tprint(\"-\" * 50)\n",
    "\n",
    "\t# Codificar Variável Alvo (y) para formato numérico 0..N-1\n",
    "\tle = LabelEncoder()\n",
    "\ty_codificada = le.fit_transform(y_original) # Converte strings/categorias para 0, 1, 2...\n",
    "\n",
    "\tclass_names = le.classes_ # Salva os nomes originais das classes\n",
    "\t\n",
    "\tprint(f\"\\nCodificação da Variável Alvo '{nome_coluna_alvo}':\")\n",
    "\tprint(\"Classes originais:\", le.classes_)\n",
    "\tprint(\"Classes codificadas (únicas):\", np.unique(y_codificada))\n",
    "\tprint(\"Alvo (y) após codificação (primeiras ocorrências):\", y_codificada[:5])\n",
    "\tprint(\"-\" * 50)\n",
    "\n",
    "\t# Dividir em Dados de Treino e Teste\n",
    "\ttry:\n",
    "\n",
    "\t\tX_train, X_test, y_train, y_test = train_test_split(X_codificado, y_codificada, test_size=0.25, random_state=42, stratify=y_codificada)\n",
    "\n",
    "\texcept ValueError:\n",
    "\n",
    "\t\tprint(\"Aviso: Não foi possível usar 'stratify'. Tentando sem.\")\n",
    "\n",
    "\t\tX_train, X_test, y_train, y_test = train_test_split(X_codificado, y_codificada, test_size=0.25, random_state=42)\n",
    "\n",
    "\tprint(f\"\\nDimensões dos conjuntos de treino/teste estabelecidas.\")\n",
    "\tprint(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939247e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para salvar os modelos\n",
    "caminho_rf = \"models/modelo_rf.joblib\"\n",
    "caminho_rf_optuna = \"models/modelo_rf_optuna.joblib\"\n",
    "caminho_xgb = \"models/modelo_xgb.joblib\"\n",
    "\n",
    "# Treinar e salvar cada modelo\n",
    "random_forest(X_train, y_train, feature_names, caminho_rf)\n",
    "random_forest_optuna(X_train, y_train, X_test, y_test, feature_names, caminho_rf_optuna)\n",
    "xgboost(X_train, y_train, feature_names, caminho_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4decc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_e_plotar_modelos(caminhos_modelos, X_test, y_test, class_names, feature_names):\n",
    "  \n",
    "    for nome_modelo, caminho in caminhos_modelos.items():\n",
    "        print(f\"--- Carregando e Avaliando: {nome_modelo} ---\")\n",
    "        \n",
    "        try:\n",
    "            modelo_carregado = joblib.load(caminho)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Arquivo do modelo não encontrado em: {caminho}\")\n",
    "            continue\n",
    "\n",
    "        y_pred = modelo_carregado.predict(X_test)\n",
    "\n",
    "        # Relatório de Classificação\n",
    "        print(\"\\nRelatório de Classificação:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "        # Matriz de Confusão\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Matriz de Confusão - {nome_modelo}')\n",
    "        plt.ylabel('Verdadeiro')\n",
    "        plt.xlabel('Previsto')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Importância das Features\n",
    "        if hasattr(modelo_carregado, 'feature_importances_'):\n",
    "            importances = modelo_carregado.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            num_features_to_plot = min(len(feature_names), 20)\n",
    "            \n",
    "            plt.figure(figsize=(12, max(6, num_features_to_plot * 0.3)))\n",
    "            plt.title(f\"Importância das Features - {nome_modelo} (Top {num_features_to_plot})\")\n",
    "            sns.barplot(x=importances[indices][:num_features_to_plot],\n",
    "                        y=[feature_names[i] for i in indices[:num_features_to_plot]])\n",
    "            plt.xlabel(\"Importância Relativa\")\n",
    "            plt.ylabel(\"Feature\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Dicionário com os nomes dos modelos e os caminhos onde foram salvos\n",
    "caminhos_dos_modelos = {\n",
    "    \"Random Forest\": \"models/modelo_rf.joblib\",\n",
    "    \"Random Forest (Optuna)\": \"models/modelo_rf_optuna.joblib\",\n",
    "    \"XGBoost\": \"models/modelo_xgb.joblib\"\n",
    "}\n",
    "\n",
    "# Carregar os modelos e gerar os gráficos\n",
    "carregar_e_plotar_modelos(caminhos_dos_modelos, X_test, y_test, class_names, feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
